{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:28:14.256745Z",
     "iopub.status.busy": "2025-07-13T16:28:14.256366Z",
     "iopub.status.idle": "2025-07-13T16:28:26.297865Z",
     "shell.execute_reply": "2025-07-13T16:28:26.297058Z",
     "shell.execute_reply.started": "2025-07-13T16:28:14.256723Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe_connected'\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_auc_score, roc_curve\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:28:26.299470Z",
     "iopub.status.busy": "2025-07-13T16:28:26.299033Z",
     "iopub.status.idle": "2025-07-13T16:28:26.303251Z",
     "shell.execute_reply": "2025-07-13T16:28:26.302522Z",
     "shell.execute_reply.started": "2025-07-13T16:28:26.299450Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Experiments***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:28:26.304249Z",
     "iopub.status.busy": "2025-07-13T16:28:26.303999Z",
     "iopub.status.idle": "2025-07-13T16:28:26.329524Z",
     "shell.execute_reply": "2025-07-13T16:28:26.328967Z",
     "shell.execute_reply.started": "2025-07-13T16:28:26.304226Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "MAX_LEN = 64\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "TRAINING_FILE = \"/kaggle/input/data-splits-20-06-2025/train_set.csv\"\n",
    "VALIDATION_FILE = \"/kaggle/input/data-splits-20-06-2025/validation_set.csv\"\n",
    "TEST_FILE = \"/kaggle/input/data-splits-20-06-2025/test_set.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:28:26.331116Z",
     "iopub.status.busy": "2025-07-13T16:28:26.330851Z",
     "iopub.status.idle": "2025-07-13T16:28:26.350298Z",
     "shell.execute_reply": "2025-07-13T16:28:26.349630Z",
     "shell.execute_reply.started": "2025-07-13T16:28:26.331099Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, product1, product2, target, tokenizer, max_len=MAX_LEN):\n",
    "        self.product1 = product1\n",
    "        self.product2 = product2\n",
    "        self.target = target\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.product1)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        title1 = str(self.product1[item])\n",
    "        title2 = str(self.product2[item])\n",
    "\n",
    "        # Tokenize both titles (product1 and product2) and pad/truncate to max_len\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            title1, title2, \n",
    "            add_special_tokens=True,  # Adds [CLS] and [SEP]\n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            return_attention_mask=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Prepare input tensors\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        label = torch.tensor(self.target[item], dtype=torch.float)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:28:28.328738Z",
     "iopub.status.busy": "2025-07-13T16:28:28.328446Z",
     "iopub.status.idle": "2025-07-13T16:28:28.338405Z",
     "shell.execute_reply": "2025-07-13T16:28:28.337700Z",
     "shell.execute_reply.started": "2025-07-13T16:28:28.328714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the custom loss function\n",
    "def loss_fn(outputs, labels):\n",
    "    return nn.BCEWithLogitsLoss()(outputs, labels)\n",
    "\n",
    "# Training function\n",
    "def train_loop(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.squeeze(-1)  # Ensure shape [batch_size]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(logits, labels)  # Using BCEWithLogitsLoss\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss += [loss.item()]\n",
    "        \n",
    "        # Calculate accuracy based on logits (thresholded)\n",
    "        preds = torch.sigmoid(logits).round()  # Convert logits to binary predictions\n",
    "        total_correct += (preds.squeeze() == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss) \n",
    "    accuracy = total_correct / total_samples  \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def test_loop(data_loader, model, device, return_details=False):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_preds, all_labels, all_losses = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.squeeze(-1)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = probs.round()\n",
    "\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_losses.extend(nn.BCELoss(reduction=\"none\")(probs, labels).cpu().numpy())\n",
    "\n",
    "    avg_val_loss = np.mean(val_loss)\n",
    "    accuracy = total_correct / total_samples\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    if return_details:\n",
    "        return avg_val_loss, accuracy, f1, all_preds, all_labels, all_losses\n",
    "    \n",
    "    return avg_val_loss, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T17:10:05.434268Z",
     "iopub.status.busy": "2025-07-13T17:10:05.433466Z",
     "iopub.status.idle": "2025-07-13T17:15:22.788663Z",
     "shell.execute_reply": "2025-07-13T17:15:22.787879Z",
     "shell.execute_reply.started": "2025-07-13T17:10:05.434244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff644bad06d94c40a0a266e6d56e67ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8a72da140946e7a6fc1bccf9af925b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ec0edf16524a27bfe92c46cac69869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001385e5f8ac495ca3f537c78b70884d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9145d654ff405a82dcc08c4e9ddb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training: 100%|██████████| 303/303 [00:59<00:00,  5.11it/s]\n",
      "Evaluating: 100%|██████████| 76/76 [00:02<00:00, 32.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 Train Loss: 0.334 Train Accuracy: 84.73 %  Val Loss 0.209 Val Accuracy 91.56 %  Val F1 91.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 303/303 [01:00<00:00,  5.05it/s]\n",
      "Evaluating: 100%|██████████| 76/76 [00:02<00:00, 32.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5 Train Loss: 0.152 Train Accuracy: 94.6 %  Val Loss 0.141 Val Accuracy 95.7 %  Val F1 95.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 303/303 [01:00<00:00,  5.05it/s]\n",
      "Evaluating: 100%|██████████| 76/76 [00:02<00:00, 32.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5 Train Loss: 0.096 Train Accuracy: 96.64 %  Val Loss 0.144 Val Accuracy 95.53 %  Val F1 95.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 303/303 [00:59<00:00,  5.05it/s]\n",
      "Evaluating: 100%|██████████| 76/76 [00:02<00:00, 32.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5 Train Loss: 0.069 Train Accuracy: 97.81 %  Val Loss 0.148 Val Accuracy 95.7 %  Val F1 95.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 303/303 [00:59<00:00,  5.05it/s]\n",
      "Evaluating: 100%|██████████| 76/76 [00:02<00:00, 32.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5 Train Loss: 0.051 Train Accuracy: 98.37 %  Val Loss 0.142 Val Accuracy 95.78 %  Val F1 95.78\n",
      "\n",
      "Total training time: 00:05:17\n"
     ]
    }
   ],
   "source": [
    "# Main function to run training and evaluation\n",
    "def train(model_name='distilbert-base-uncased', max_len=128):\n",
    "    \n",
    "    # Start timing for training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read training and validation datasets\n",
    "    df_train = pd.read_csv(TRAINING_FILE)\n",
    "    df_valid = pd.read_csv(VALIDATION_FILE)\n",
    "\n",
    "    # Define the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Create dataset objects for training and validation\n",
    "    train_dataset = BERTDataset(\n",
    "        product1=df_train.title_1.values,\n",
    "        product2=df_train.title_2.values,\n",
    "        target=df_train.label.values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset, TRAIN_BATCH_SIZE, shuffle=True, num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = BERTDataset(\n",
    "        product1=df_valid.title_1.values,\n",
    "        product2=df_valid.title_2.values,\n",
    "        target=df_valid.label.values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset, VALID_BATCH_SIZE, num_workers=1\n",
    "    )\n",
    "\n",
    "    # Prepare the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare optimizer and scheduler\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)  \n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    max_val_accuracy = 0\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    val_f1s = []\n",
    "    \n",
    "    # Iterate over the whole dataset EPOCHS times \n",
    "    for epoch in range(EPOCHS):  \n",
    "        \n",
    "        # Per epoch training and validation loops \n",
    "        train_loss, train_accuracy = train_loop(train_data_loader, model, optimizer, device, scheduler)\n",
    "        val_loss, val_accuracy, val_f1 = test_loop(valid_data_loader, model, device)\n",
    "\n",
    "        # Store metrics for plotting \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "         # Print the training results per epoch\n",
    "        log_string = \"Epoch: {}/{} Train Loss: {} Train Accuracy: {} %  Val Loss {} Val Accuracy {} %  Val F1 {}\"\n",
    "        print(log_string.format(epoch + 1,\n",
    "                                EPOCHS, \n",
    "                                round(train_loss, 3), \n",
    "                                round(100 * train_accuracy, 2),\n",
    "                                round(val_loss, 3),\n",
    "                                round(100 * val_accuracy, 2),\n",
    "                                round(100 * val_f1, 2)\n",
    "                                ))\n",
    "\n",
    "        # Save the model with the highest validation accuracy\n",
    "        if val_accuracy > max_val_accuracy:\n",
    "            safe_model_name = model_name.replace(\"/\", \"_\")\n",
    "            torch.save(model.state_dict(), f\"{safe_model_name}_max_len_{max_len}_product_matching.pth\")\n",
    "            max_val_accuracy = val_accuracy\n",
    "\n",
    "    # Track the total training time \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    hours, rem = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(f\"\\nTotal training time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "\n",
    "    # Save the training time \n",
    "    with open(f\"training_time_{safe_model_name}_max_len_{max_len}_product_matching.txt\", \"w\") as f:\n",
    "        f.write(f\"Total training time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\\n\")\n",
    "\n",
    "    # Save statistics for plotting later\n",
    "    epochs = range(1, EPOCHS + 1)\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"epoch\": epochs,\n",
    "        \"train_loss\": train_losses,\n",
    "        \"val_loss\": val_losses,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "        \"val_accuracy\": val_accuracies,\n",
    "        \"val_f1\": val_f1s\n",
    "    })\n",
    "\n",
    "    metrics_df.to_csv(f\"training_metrics_{safe_model_name}_max_len_{max_len}_product_matching.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(model_name='distilbert-base-uncased', max_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Inference***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:39:38.853710Z",
     "iopub.status.busy": "2025-07-13T16:39:38.852543Z",
     "iopub.status.idle": "2025-07-13T16:39:38.894365Z",
     "shell.execute_reply": "2025-07-13T16:39:38.893771Z",
     "shell.execute_reply.started": "2025-07-13T16:39:38.853656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_1</th>\n",
       "      <th>title_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cyrus outdoor mobile phone cm8 solid unlocked</td>\n",
       "      <td>archos core 60s 15.2 cm 6 2 gb 16 gb dual sim ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackberry z10 3g smartphone sim free touch sc...</td>\n",
       "      <td>blackberry z10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zte blade a430 android 8gb sim free unlock...</td>\n",
       "      <td>nokia 6700 slide mobile phone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doro 7354 2404 2g uk sim free mobile phone dua...</td>\n",
       "      <td>doro 2404 blackwhite 2.4 2g unlocked sim free</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackberry 8800 mobile phone</td>\n",
       "      <td>sony xperia l1 sim free smartphone black</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>beafon sl340 eu001r bea fon sl340 red</td>\n",
       "      <td>bea fon sl340 blau</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>xiaomi redmi 6 dual sim 32gb black eu</td>\n",
       "      <td>xiaomi redmi 5 dual sim black 32gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>samsung g600 pink mobile phone</td>\n",
       "      <td>sim free nokia 7 plus 64gb mobile phone blackc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>swisstone sc330 schwarz</td>\n",
       "      <td>swisstone sc230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>huawei p10 64gb graphite black</td>\n",
       "      <td>huawei p10 plus 128gb graphite black</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1210 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title_1  \\\n",
       "0         cyrus outdoor mobile phone cm8 solid unlocked   \n",
       "1     blackberry z10 3g smartphone sim free touch sc...   \n",
       "2     new zte blade a430 android 8gb sim free unlock...   \n",
       "3     doro 7354 2404 2g uk sim free mobile phone dua...   \n",
       "4                          blackberry 8800 mobile phone   \n",
       "...                                                 ...   \n",
       "1205              beafon sl340 eu001r bea fon sl340 red   \n",
       "1206              xiaomi redmi 6 dual sim 32gb black eu   \n",
       "1207                     samsung g600 pink mobile phone   \n",
       "1208                            swisstone sc330 schwarz   \n",
       "1209                     huawei p10 64gb graphite black   \n",
       "\n",
       "                                                title_2  label  \n",
       "0     archos core 60s 15.2 cm 6 2 gb 16 gb dual sim ...      0  \n",
       "1                                        blackberry z10      1  \n",
       "2                         nokia 6700 slide mobile phone      0  \n",
       "3         doro 2404 blackwhite 2.4 2g unlocked sim free      1  \n",
       "4              sony xperia l1 sim free smartphone black      0  \n",
       "...                                                 ...    ...  \n",
       "1205                                 bea fon sl340 blau      1  \n",
       "1206                 xiaomi redmi 5 dual sim black 32gb      0  \n",
       "1207  sim free nokia 7 plus 64gb mobile phone blackc...      0  \n",
       "1208                                    swisstone sc230      0  \n",
       "1209               huawei p10 plus 128gb graphite black      0  \n",
       "\n",
       "[1210 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(TEST_FILE)\n",
    "all_labels = df_test[\"label\"].values\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:41:44.907859Z",
     "iopub.status.busy": "2025-07-13T16:41:44.907606Z",
     "iopub.status.idle": "2025-07-13T16:41:44.922348Z",
     "shell.execute_reply": "2025-07-13T16:41:44.921645Z",
     "shell.execute_reply.started": "2025-07-13T16:41:44.907843Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_inference_with_eval_and_plots(model_name, df, max_len=128, batch_size=32, label_names=None, output_dir=\"inference_outputs\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Start timing \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Prepare safe names and device\n",
    "    safe_model_name = model_name.replace(\"/\", \"_\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "    model.load_state_dict(torch.load(f\"/kaggle/working/{safe_model_name}_max_len_{max_len}_product_matching.pth\", map_location=device))\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # Initialize accumulators\n",
    "    all_preds, all_probs, all_losses = [], [], []\n",
    "    all_labels = df[\"label\"].values\n",
    "    criterion = BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    # Run inference with loss\n",
    "    for start_idx in tqdm(range(0, len(df), batch_size), desc=\"Inferencing\"):\n",
    "        batch = df.iloc[start_idx:start_idx + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            list(batch[\"title_1\"]),\n",
    "            list(batch[\"title_2\"]),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = torch.tensor(batch[\"label\"].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        if model_name in [\"roberta-base\", \"microsoft/deberta-v3-base\"]:\n",
    "            inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).long()\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_losses.extend(loss.cpu().numpy())\n",
    "\n",
    "    # End timing \n",
    "    end_time = time.time()\n",
    "    inference_duration = end_time - start_time\n",
    "\n",
    "    all_preds = np.array(all_preds).astype(int)\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_losses = np.array(all_losses)\n",
    "\n",
    "    # ------------------ Metrics ------------------\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "\n",
    "    metrics_df = pd.DataFrame([{\n",
    "        \"model\": model_name,\n",
    "        \"max_len\": max_len,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_score\": f1,\n",
    "        \"inference_time_sec\": round(inference_duration, 2)\n",
    "    }])\n",
    "    metrics_df.to_csv(f\"{output_dir}/{safe_model_name}_maxlen{max_len}_metrics_product_matching.csv\", index=False)\n",
    "\n",
    "    # ------------------ Plots ------------------\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    disp.plot(ax=ax, cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix ({safe_model_name}, max_len = {max_len})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{safe_model_name}_maxlen{max_len}_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    df_report = pd.DataFrame(report).transpose().drop([\"accuracy\", \"macro avg\", \"weighted avg\"], errors=\"ignore\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df_report.iloc[:, :-1], annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar=True)\n",
    "    plt.title(f\"Classification Report Heatmap ({safe_model_name}, max_len = {max_len})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{safe_model_name}_maxlen{max_len}_classification_heatmap.png\")\n",
    "    plt.close()\n",
    "\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    per_class_df = report_df.iloc[:-3]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=per_class_df.index, y=per_class_df[\"f1-score\"], palette=\"viridis\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.title(f\"F1 Scores per Category ({safe_model_name}, max_len = {max_len})\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{safe_model_name}_maxlen{max_len}_f1_per_category.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # ------------------ Error Analysis ------------------\n",
    "    df_errors = df.copy()\n",
    "    df_errors[\"predicted_label\"] = all_preds\n",
    "    df_errors[\"probability\"] = all_probs\n",
    "    df_errors[\"loss\"] = all_losses\n",
    "    df_errors_sorted = df_errors.sort_values(by=\"loss\", ascending=False)\n",
    "\n",
    "    # Display top-20 most confusing predictions\n",
    "    print(\"\\nTop 20 Most Confusing Predictions (by loss):\")\n",
    "    display(df_errors_sorted.head(20))\n",
    "\n",
    "    # Save them to CSV\n",
    "    df_errors_sorted.to_csv(f\"{output_dir}/{safe_model_name}_maxlen{max_len}_top_test_errors.csv\", index=False)\n",
    "\n",
    "    return all_preds, all_probs, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T17:15:38.048138Z",
     "iopub.status.busy": "2025-07-13T17:15:38.047820Z",
     "iopub.status.idle": "2025-07-13T17:15:40.980245Z",
     "shell.execute_reply": "2025-07-13T17:15:40.979497Z",
     "shell.execute_reply.started": "2025-07-13T17:15:38.048093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Inferencing: 100%|██████████| 38/38 [00:01<00:00, 25.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Most Confusing Predictions (by loss):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_1</th>\n",
       "      <th>title_2</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>probability</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>motorola moto g 5.7 single sim 4g 3gb 32gb 300...</td>\n",
       "      <td>sim free motorola moto g6 32gb mobile phone de...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>6.607663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>motorola moto g 5.7 single sim 4g 3gb 32gb 300...</td>\n",
       "      <td>motorola moto g6 indigo 5.7 32gb 4g unlocked s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>5.612900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>htc one x pj46100 16gb grey unlocked smartphon...</td>\n",
       "      <td>htc one x grey</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995852</td>\n",
       "      <td>5.485095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>sim free sony xperia xz1 mobile phone blue</td>\n",
       "      <td>sim free sony xperia xz1 64gb mobile phone black</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995722</td>\n",
       "      <td>5.454176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>doro phoneeasy 609l black</td>\n",
       "      <td>doro phoneeasy 609 black</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>5.372153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>nokia 3310 mobile phone in azure</td>\n",
       "      <td>nokia 3310 3g</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995218</td>\n",
       "      <td>5.342935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>huawei p smart 32gb sim free 4g lte smartphone...</td>\n",
       "      <td>sim free huawei p smart 32gb mobile phone black</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994877</td>\n",
       "      <td>5.274025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>oneplus 5t a5010 64gb dual sim factory unlocke...</td>\n",
       "      <td>oneplus 5t midnight black 64gb6gb 4g dual sim ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>4.988318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>oneplus 5t midnight black 64gb6gb 4g dual sim ...</td>\n",
       "      <td>oneplus 5t a5010 64gb dual sim factory unlocke...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>4.963792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>lg k10 2017 m250n 13.5 cm 5.3 16 gb 13 mp andr...</td>\n",
       "      <td>lg m250e 13 46 cm 5 3 zoll smartphone k10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991164</td>\n",
       "      <td>4.728958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>nokia 130 mobile phone in red</td>\n",
       "      <td>sim free nokia 130 2017 mobile phone red</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988034</td>\n",
       "      <td>4.425706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>sim free motorola moto g6 32gb mobile phone de...</td>\n",
       "      <td>motorola moto g 5.7 single sim 4g 3gb 32gb 300...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013266</td>\n",
       "      <td>4.322570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>13.208 cm 5.2 1280 x 720 ips gsmwcdmalte qualc...</td>\n",
       "      <td>beafon sl150 eu001b bea fon sl150 black unlocked</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.986434</td>\n",
       "      <td>4.300184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>samsung galaxy note 9 128gb smartphone black</td>\n",
       "      <td>samsung galaxy note9 duos 128gb midnight black</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.981435</td>\n",
       "      <td>3.986459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>htc u11 uk sim free smartphone amazing silver</td>\n",
       "      <td>htc u11 life uk sim free smartphone brilliant ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019459</td>\n",
       "      <td>3.939467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>samsung galaxy s8 64gb orchid grey</td>\n",
       "      <td>samsung galaxy s8 64 gb orchid grey</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979552</td>\n",
       "      <td>3.889848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>sony xperia xa2 ultra silver</td>\n",
       "      <td>sony xperia xa2 ultra dual sim silver</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978150</td>\n",
       "      <td>3.823537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>nokia 105 blue</td>\n",
       "      <td>nokia 105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.971730</td>\n",
       "      <td>3.565961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>xiaomi redmi note 5 3gb ram 32gb rom red eu ve...</td>\n",
       "      <td>xiaomi mi a2 dual sim 128gb black eu</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>3.434946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>doro 6030 easy to use camera phone with large ...</td>\n",
       "      <td>doro mobile 6030 graphitewhite 2.4in 16mb vga ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.966373</td>\n",
       "      <td>3.392421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title_1  \\\n",
       "489   motorola moto g 5.7 single sim 4g 3gb 32gb 300...   \n",
       "922   motorola moto g 5.7 single sim 4g 3gb 32gb 300...   \n",
       "340   htc one x pj46100 16gb grey unlocked smartphon...   \n",
       "59           sim free sony xperia xz1 mobile phone blue   \n",
       "181                           doro phoneeasy 609l black   \n",
       "1044                   nokia 3310 mobile phone in azure   \n",
       "722   huawei p smart 32gb sim free 4g lte smartphone...   \n",
       "826   oneplus 5t a5010 64gb dual sim factory unlocke...   \n",
       "378   oneplus 5t midnight black 64gb6gb 4g dual sim ...   \n",
       "791   lg k10 2017 m250n 13.5 cm 5.3 16 gb 13 mp andr...   \n",
       "1187                      nokia 130 mobile phone in red   \n",
       "1049  sim free motorola moto g6 32gb mobile phone de...   \n",
       "591   13.208 cm 5.2 1280 x 720 ips gsmwcdmalte qualc...   \n",
       "1172       samsung galaxy note 9 128gb smartphone black   \n",
       "930       htc u11 uk sim free smartphone amazing silver   \n",
       "313                  samsung galaxy s8 64gb orchid grey   \n",
       "472                        sony xperia xa2 ultra silver   \n",
       "790                                      nokia 105 blue   \n",
       "346   xiaomi redmi note 5 3gb ram 32gb rom red eu ve...   \n",
       "873   doro 6030 easy to use camera phone with large ...   \n",
       "\n",
       "                                                title_2  label  \\\n",
       "489   sim free motorola moto g6 32gb mobile phone de...      1   \n",
       "922   motorola moto g6 indigo 5.7 32gb 4g unlocked s...      1   \n",
       "340                                      htc one x grey      0   \n",
       "59     sim free sony xperia xz1 64gb mobile phone black      0   \n",
       "181                            doro phoneeasy 609 black      1   \n",
       "1044                                      nokia 3310 3g      0   \n",
       "722     sim free huawei p smart 32gb mobile phone black      0   \n",
       "826   oneplus 5t midnight black 64gb6gb 4g dual sim ...      1   \n",
       "378   oneplus 5t a5010 64gb dual sim factory unlocke...      1   \n",
       "791           lg m250e 13 46 cm 5 3 zoll smartphone k10      0   \n",
       "1187           sim free nokia 130 2017 mobile phone red      0   \n",
       "1049  motorola moto g 5.7 single sim 4g 3gb 32gb 300...      1   \n",
       "591    beafon sl150 eu001b bea fon sl150 black unlocked      0   \n",
       "1172     samsung galaxy note9 duos 128gb midnight black      0   \n",
       "930   htc u11 life uk sim free smartphone brilliant ...      1   \n",
       "313                 samsung galaxy s8 64 gb orchid grey      0   \n",
       "472               sony xperia xa2 ultra dual sim silver      0   \n",
       "790                                           nokia 105      0   \n",
       "346                xiaomi mi a2 dual sim 128gb black eu      1   \n",
       "873   doro mobile 6030 graphitewhite 2.4in 16mb vga ...      0   \n",
       "\n",
       "      predicted_label  probability      loss  \n",
       "489                 0     0.001350  6.607663  \n",
       "922                 0     0.003650  5.612900  \n",
       "340                 1     0.995852  5.485095  \n",
       "59                  1     0.995722  5.454176  \n",
       "181                 0     0.004644  5.372153  \n",
       "1044                1     0.995218  5.342935  \n",
       "722                 1     0.994877  5.274025  \n",
       "826                 0     0.006817  4.988318  \n",
       "378                 0     0.006986  4.963792  \n",
       "791                 1     0.991164  4.728958  \n",
       "1187                1     0.988034  4.425706  \n",
       "1049                0     0.013266  4.322570  \n",
       "591                 1     0.986434  4.300184  \n",
       "1172                1     0.981435  3.986459  \n",
       "930                 0     0.019459  3.939467  \n",
       "313                 1     0.979552  3.889848  \n",
       "472                 1     0.978150  3.823537  \n",
       "790                 1     0.971730  3.565961  \n",
       "346                 0     0.032227  3.434946  \n",
       "873                 1     0.966373  3.392421  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds, probs, metrics_df = run_inference_with_eval_and_plots(\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    df=df_test,\n",
    "    max_len=128,\n",
    "    batch_size=32,\n",
    "    output_dir=\"distilbert_maxlen128\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7718616,
     "sourceId": 12249997,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
