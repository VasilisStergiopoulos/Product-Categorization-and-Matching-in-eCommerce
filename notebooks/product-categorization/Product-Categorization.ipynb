{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:20:09.558527Z",
     "iopub.status.busy": "2025-07-08T19:20:09.558171Z",
     "iopub.status.idle": "2025-07-08T19:20:21.604206Z",
     "shell.execute_reply": "2025-07-08T19:20:21.603653Z",
     "shell.execute_reply.started": "2025-07-08T19:20:09.558499Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe_connected'\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T20:56:43.084179Z",
     "iopub.status.busy": "2025-07-08T20:56:43.083872Z",
     "iopub.status.idle": "2025-07-08T20:56:43.088119Z",
     "shell.execute_reply": "2025-07-08T20:56:43.087480Z",
     "shell.execute_reply.started": "2025-07-08T20:56:43.084158Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Experiments***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:20:25.856632Z",
     "iopub.status.busy": "2025-07-08T19:20:25.855648Z",
     "iopub.status.idle": "2025-07-08T19:20:25.860248Z",
     "shell.execute_reply": "2025-07-08T19:20:25.859493Z",
     "shell.execute_reply.started": "2025-07-08T19:20:25.856606Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "MAX_LEN = 64\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "TRAINING_FILE = \"/kaggle/input/data-splits-product-categorization/train_set_product_categorization.csv\"\n",
    "VALIDATION_FILE = \"/kaggle/input/data-splits-product-categorization/validation_set_product_categorization.csv\"\n",
    "TEST_FILE = \"/kaggle/input/data-splits-product-categorization/test_set_product_categorization.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:20:34.081080Z",
     "iopub.status.busy": "2025-07-08T19:20:34.080547Z",
     "iopub.status.idle": "2025-07-08T19:20:34.087076Z",
     "shell.execute_reply": "2025-07-08T19:20:34.086228Z",
     "shell.execute_reply.started": "2025-07-08T19:20:34.081057Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, title, target, tokenizer, max_len=MAX_LEN):\n",
    "        self.title = title\n",
    "        self.target = target\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        title = str(self.title[item])\n",
    "\n",
    "        # Tokenize title and pad/truncate to max_len\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            add_special_tokens=True,  # Adds [CLS] and [SEP]\n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            return_attention_mask=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Prepare input tensors\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        label = torch.tensor(self.target[item], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:20:37.191061Z",
     "iopub.status.busy": "2025-07-08T19:20:37.190130Z",
     "iopub.status.idle": "2025-07-08T19:20:37.206778Z",
     "shell.execute_reply": "2025-07-08T19:20:37.205990Z",
     "shell.execute_reply.started": "2025-07-08T19:20:37.191022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training function\n",
    "def train_loop(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # In the train loop:\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss.append(loss.item())  \n",
    "\n",
    "        # Calculate accuracy based on logits (thresholded)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_correct += (preds.squeeze() == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    " \n",
    "    avg_loss = np.mean(epoch_loss)  # Calculate average loss\n",
    "    accuracy = total_correct / total_samples  # Calculate accuracy\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def test_loop(data_loader, model, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Dictionary to store per-category stats\n",
    "    category_correct = defaultdict(int)\n",
    "    category_total = defaultdict(int)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            # Update totals\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Update per-category counts\n",
    "            for label, pred in zip(labels.cpu().numpy(), preds.cpu().numpy()):\n",
    "                category_total[label] += 1\n",
    "                if label == pred:\n",
    "                    category_correct[label] += 1\n",
    "\n",
    "    # Per-category accuracy\n",
    "    category_accuracies = {\n",
    "        label: category_correct[label] / category_total[label]\n",
    "        for label in category_total\n",
    "    }\n",
    "\n",
    "    avg_val_loss = np.mean(val_loss)\n",
    "    accuracy = total_correct / total_samples\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    return avg_val_loss, accuracy, f1, category_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T20:36:48.793579Z",
     "iopub.status.busy": "2025-07-08T20:36:48.792940Z",
     "iopub.status.idle": "2025-07-08T20:51:35.870888Z",
     "shell.execute_reply": "2025-07-08T20:51:35.869959Z",
     "shell.execute_reply.started": "2025-07-08T20:36:48.793557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training:   0%|          | 0/883 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 883/883 [02:48<00:00,  5.23it/s]\n",
      "Evaluating:   0%|          | 0/221 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 221/221 [00:07<00:00, 30.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 Train loss: 0.243 Training accuracy: 93.7 %  Val loss 0.064 Val accuracy 98.39 %  Val F1 98.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/883 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 883/883 [02:48<00:00,  5.23it/s]\n",
      "Evaluating:   0%|          | 0/221 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 221/221 [00:07<00:00, 30.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5 Train loss: 0.046 Training accuracy: 98.83 %  Val loss 0.047 Val accuracy 98.87 %  Val F1 98.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/883 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 883/883 [02:48<00:00,  5.23it/s]\n",
      "Evaluating:   0%|          | 0/221 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 221/221 [00:07<00:00, 30.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5 Train loss: 0.022 Training accuracy: 99.45 %  Val loss 0.041 Val accuracy 99.01 %  Val F1 99.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/883 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 883/883 [02:49<00:00,  5.22it/s]\n",
      "Evaluating:   0%|          | 0/221 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 221/221 [00:07<00:00, 30.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5 Train loss: 0.012 Training accuracy: 99.76 %  Val loss 0.035 Val accuracy 99.26 %  Val F1 99.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/883 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 883/883 [02:49<00:00,  5.22it/s]\n",
      "Evaluating:   0%|          | 0/221 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 221/221 [00:07<00:00, 30.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5 Train loss: 0.008 Training accuracy: 99.85 %  Val loss 0.035 Val accuracy 99.32 %  Val F1 99.32\n",
      "\n",
      "Total training time: 00:14:47\n"
     ]
    }
   ],
   "source": [
    "# Main function to run training and evaluation\n",
    "def train(model_name='roberta-base-uncased', max_len=64):\n",
    "\n",
    "    # Start timing for training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read training and validation datasets\n",
    "    df_train = pd.read_csv(TRAINING_FILE)\n",
    "    df_valid = pd.read_csv(VALIDATION_FILE)\n",
    "\n",
    "    # Define the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Create dataset objects for training and validation\n",
    "    train_dataset = BERTDataset(\n",
    "        title=df_train.title.values,\n",
    "        target=df_train.category_label_encoded.values,\n",
    "        tokenizer = tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset, TRAIN_BATCH_SIZE, shuffle=True, num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = BERTDataset(\n",
    "        title=df_valid.title.values,\n",
    "        target=df_valid.category_label_encoded.values,\n",
    "        tokenizer = tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset, VALID_BATCH_SIZE, num_workers=1\n",
    "    )\n",
    "\n",
    "    # Prepare the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=10)\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare optimizer and scheduler\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)  \n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    max_val_accuracy = 0\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    val_f1s = []\n",
    "    \n",
    "    # Iterate over the whole dataset EPOCHS times \n",
    "    for epoch in range(EPOCHS):  \n",
    "        \n",
    "        # Per epoch training and validation loops \n",
    "        train_loss, train_accuracy = train_loop(train_data_loader, model, optimizer, device, scheduler)\n",
    "        val_loss, val_accuracy, val_f1, per_category_accuracy = test_loop(valid_data_loader, model, device)\n",
    "\n",
    "        # Store metrics for plotting \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "         # Print the training results per epoch\n",
    "        log_string = \"Epoch: {}/{} Train loss: {} Training accuracy: {} %  Val loss {} Val accuracy {} %  Val F1 {}\"\n",
    "        print(log_string.format(epoch + 1,\n",
    "                                EPOCHS, \n",
    "                                round(train_loss, 3), \n",
    "                                round(100 * train_accuracy, 2),\n",
    "                                round(val_loss, 3),\n",
    "                                round(100 * val_accuracy, 2),\n",
    "                                round(100 * val_f1, 2)\n",
    "                                ))\n",
    "\n",
    "        # Print per-category accuracy\n",
    "        # print(\"Per-category accuracy:\")\n",
    "        # for category_id, acc in sorted(per_category_accuracy.items()):\n",
    "        #     category_name = label_encoder.inverse_transform([category_id])[0]\n",
    "        #     print(f\"  {category_name} (ID {category_id}): {round(100 * acc, 2)}%\")\n",
    "\n",
    "        \n",
    "        # Save the model with the highest validation accuracy\n",
    "        if val_accuracy > max_val_accuracy:\n",
    "            safe_model_name = model_name.replace(\"/\", \"_\")\n",
    "            torch.save(model.state_dict(), f\"{safe_model_name}_max_len_{max_len}_product_categorization.pth\")\n",
    "            max_val_accuracy = val_accuracy\n",
    "\n",
    "\n",
    "    # Track the total training time \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    hours, rem = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(f\"\\nTotal training time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "\n",
    "    # Save the training time \n",
    "    with open(f\"training_time_{safe_model_name}_max_len_{max_len}.txt\", \"w\") as f:\n",
    "        f.write(f\"Total training time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\\n\")\n",
    "\n",
    "    # Save statistics for plotting later\n",
    "    epochs = range(1, EPOCHS + 1)\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"epoch\": epochs,\n",
    "        \"train_loss\": train_losses,\n",
    "        \"val_loss\": val_losses,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "        \"val_accuracy\": val_accuracies,\n",
    "        \"val_f1\": val_f1s\n",
    "    })\n",
    "\n",
    "    metrics_df.to_csv(f\"training_metrics_{safe_model_name}_max_len_{max_len}.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(model_name='roberta-base-uncased', max_len=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Inference***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T20:51:43.935025Z",
     "iopub.status.busy": "2025-07-08T20:51:43.934345Z",
     "iopub.status.idle": "2025-07-08T20:51:43.957111Z",
     "shell.execute_reply": "2025-07-08T20:51:43.956473Z",
     "shell.execute_reply.started": "2025-07-08T20:51:43.934996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category_label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>constructa einbau k hlschrank ck 60430</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siemens gs36nvi30g freestanding freezer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bosch kgn36vw35g exxcel frost free fridge free...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blomberg lwf29441w 1400 spin 9kg washing machine</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sony slt a77 mark ii geh use ilca77m2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>iq700 wm14yh79gb 9kg 1400 spin washing machine</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3527</th>\n",
       "      <td>smeg right hand hinge free standing fridge fre...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3528</th>\n",
       "      <td>htc wildfire s sim free mobile phone silverwhite</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>siemens sn558s02me geschirrsp ler integriert 60cm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>jvc lt 39c770 39 smart led tv</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3531 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                constructa einbau k hlschrank ck 60430   \n",
       "1               siemens gs36nvi30g freestanding freezer   \n",
       "2     bosch kgn36vw35g exxcel frost free fridge free...   \n",
       "3      blomberg lwf29441w 1400 spin 9kg washing machine   \n",
       "4                 sony slt a77 mark ii geh use ilca77m2   \n",
       "...                                                 ...   \n",
       "3526     iq700 wm14yh79gb 9kg 1400 spin washing machine   \n",
       "3527  smeg right hand hinge free standing fridge fre...   \n",
       "3528   htc wildfire s sim free mobile phone silverwhite   \n",
       "3529  siemens sn558s02me geschirrsp ler integriert 60cm   \n",
       "3530                      jvc lt 39c770 39 smart led tv   \n",
       "\n",
       "      category_label_encoded  \n",
       "0                          5  \n",
       "1                          3  \n",
       "2                          4  \n",
       "3                          9  \n",
       "4                          1  \n",
       "...                      ...  \n",
       "3526                       9  \n",
       "3527                       4  \n",
       "3528                       7  \n",
       "3529                       2  \n",
       "3530                       8  \n",
       "\n",
       "[3531 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(TEST_FILE)\n",
    "all_labels = df_test[\"category_label_encoded\"].values\n",
    "label_names = ['CPUs', 'Digital Cameras', 'Dishwashers', 'Freezers',\n",
    "       'Fridge Freezers', 'Fridges', 'Microwaves', 'Mobile Phones', 'TVs',\n",
    "        'Washing Machines']\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T20:51:49.062477Z",
     "iopub.status.busy": "2025-07-08T20:51:49.062151Z",
     "iopub.status.idle": "2025-07-08T20:51:49.077097Z",
     "shell.execute_reply": "2025-07-08T20:51:49.076484Z",
     "shell.execute_reply.started": "2025-07-08T20:51:49.062455Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_inference_with_eval_and_plots(model_name, df, max_len=64, batch_size=32, label_names=None, output_dir=\"inference_outputs\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare safe names and device\n",
    "    safe_model_name = model_name.replace(\"/\", \"_\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=10)\n",
    "    model.load_state_dict(torch.load(f\"/kaggle/working/{safe_model_name}_max_len_{max_len}_product_categorization.pth\", map_location=device))\n",
    "    model.to(device).eval()\n",
    "\n",
    "    all_preds, all_probs, all_labels = [], [], df[\"category_label_encoded\"].values  \n",
    "\n",
    "    for start_idx in tqdm(range(0, len(df), batch_size), desc=\"Inferencing\"):\n",
    "        batch = df.iloc[start_idx:start_idx + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            list(batch[\"title\"]),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        if model_name in [\"roberta-base\", \"microsoft/deberta-v3-base\"]:\n",
    "            inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    report = classification_report(all_labels, all_preds, target_names=label_names, output_dict=True)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"model\": model_name,\n",
    "        \"max_len\": max_len,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_score\": f1\n",
    "    })\n",
    "    \n",
    "    metrics_df.to_csv(f\"{output_dir}/{safe_model_name}_maxlen{max_len}_metrics.csv\", index=False)\n",
    "\n",
    "    # Plot: Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", xticks_rotation=45)\n",
    "    plt.title(f\"Confusion Matrix ({safe_model_name}, max_len = {max_len})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{safe_model_name}_maxlen{max_len}_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Classification Report Heatmap\n",
    "    df_report = pd.DataFrame(report).transpose().drop([\"accuracy\", \"macro avg\", \"weighted avg\"], errors=\"ignore\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df_report.iloc[:, :-1], annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar=True)\n",
    "    plt.title(f\"Classification Report Heatmap ({safe_model_name}, max_len = {max_len})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{safe_model_name}_maxlen{max_len}_classification_heatmap.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: per class F1 scores\n",
    "    report_dict = classification_report(all_labels, all_preds, target_names=label_names, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    per_class_df = report_df.iloc[:-3]  # Excludes 'accuracy', 'macro avg', 'weighted avg'\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=per_class_df.index, y=per_class_df[\"f1-score\"], palette=\"viridis\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.title(f\"F1 Scores per Category ({safe_model_name}, max_len = {max_len})\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{safe_model_name}_maxlen{max_len}_f1_per_category.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Optionally return\n",
    "    return all_preds, all_probs, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T20:51:50.943963Z",
     "iopub.status.busy": "2025-07-08T20:51:50.943676Z",
     "iopub.status.idle": "2025-07-08T20:51:57.545900Z",
     "shell.execute_reply": "2025-07-08T20:51:57.545303Z",
     "shell.execute_reply.started": "2025-07-08T20:51:50.943943Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Inferencing: 100%|██████████| 111/111 [00:04<00:00, 26.79it/s]\n"
     ]
    }
   ],
   "source": [
    "preds, probs, metrics = run_inference_with_eval_and_plots(\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    df=df_test,\n",
    "    max_len=64,\n",
    "    batch_size=32,\n",
    "    label_names=label_names,\n",
    "    output_dir=\"bert_maxlen64\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7610326,
     "sourceId": 12089302,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7796104,
     "sourceId": 12364944,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
